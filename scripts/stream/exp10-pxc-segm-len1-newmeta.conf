################################################
### CONFIGURATION FILE FOR AN SMT EXPERIMENT ###
################################################

[META]
experimenter-script = /work/moses-stream/scripts/stream/experiment.perl.morph.limit.moses.both.decoder-segmenter 
experimenter-meta = /work/moses-stream/scripts/stream/experiment.meta.morph.cdec-jane.both.decoder-segmenter


[GENERAL]
### directory in which experiment is run
#
working-dir = /work/stream/ems/
external-bin-dir = /work/bin

# specification of the language pair
input-extension = ar
output-extension = en
pair-extension = ar-en

## phrase chunker settings
input-chunker = /work/moses-stream/scripts/stream/chunk-PX.sh

## segmenter settings
input-segmenter = /work/moses-stream/scripts/stream/segment-phrase_boundary.sh
output-weaver = /work/moses-stream/scripts/stream/weave-segments.perl
input-segmenter-settings = "" 
### directories that contain tools and data
# 
# moses
moses-src-dir = /work/moses-2013-09-05
#
# moses scripts
moses-script-dir = /work/moses-2013-09-05/scripts
#
# srilm
srilm-dir = /work/srilm-1.6/bin/i686-m64
#
# data
nist-training = /work/NIST12/
#iwslt-training = /work/IWSLT2012/
iwslt-training = /work/IWSLT2013
#amara-training = /work/AMARA/corpus-MTDict/corpus

### basic tools
#
# moses decoder
decoder = $moses-src-dir/bin/moses 

# conversion of phrase table into binary on-disk format
ttable-binarizer = $moses-src-dir/bin/processPhraseTable

# conversion of rule table into binary on-disk format
#ttable-binarizer = "$moses-src-dir/bin/CreateOnDiskPt 1 1 5 100 2"
input-normalizer-nb = "$nist-training/scripts/process_arabic_stanford_atb.sh"
output-normalizer = "/work/AMARA/scripts/clean-text-qcri-no-dash-en-v3.perl "
#input-normalizer-wrap = yes

# tokenizers - comment out if all your data is already tokenized
input-tokenizer = $nist-training/scripts/dummy-tokenizer.pl
output-tokenizer = "$moses-script-dir/tokenizer/tokenizer.perl -a -l $output-extension"

detokenizer = "$moses-script-dir/tokenizer/detokenizer.perl -l en"
#detokenizer-ar = "/work/IWSLT2013/scripts/detok_arabic_mada_atb.sh"

# truecasers - comment out if you do not use the truecaser
#input-truecaser = $moses-script-dir/recaser/truecase.perl
output-truecaser = $moses-script-dir/recaser/truecase2.perl
detruecaser = $moses-script-dir/recaser/detruecase.perl


### cdec parameters
##cdec-decoder = "/work/cdec/training/utils/decode.pl --jobs 10 "
##cdec-decoder-settings = "-K 500 -P"
#cdec-tuner = "/work/cdec/training/dpmert/dpmert.pl --jobs 10 --iterations 30"
##cdec-tuner = "/work/cdec/training/mira/mira.py --jobs 10 --max-iterations 10"
##cdec-defaults-w = "/work/IWSLT2013/scripts/generatedefcdec.pl"
##cdec-defaults-ini = "/work/IWSLT2013/scripts/generatedefcini.pl"
##cdec-devset-maker = "/work/IWSLT2013/scripts/generatedevset.sh"
##use-cdec = 1

### generic parallelizer for cluster and multi-core machines
# you may specify a script that allows the parallel execution
# parallizable steps (see meta file). you also need specify 
# the number of jobs (cluster) or cores (multicore)
#
#generic-parallelizer = $moses-script-dir/ems/support/generic-parallelizer.perl
#generic-parallelizer = $moses-script-dir/ems/support/generic-multicore-parallelizer.perl

### cluster settings (if run on a cluster machine)
# number of jobs to be submitted in parallel
#
#jobs = 10

# arguments to qsub when scheduling a job
#qsub-settings = ""

# project for priviledges and usage accounting 
#qsub-project = iccs_smt

# memory and time 
#qsub-memory = 4
#qsub-hours = 48

### multi-core settings
# when the generic parallelizer is used, the number of cores
# specified here 
#cores = 20

#################################################################
# PARALLEL CORPUS PREPARATION: 
# create a tokenized, sentence-aligned corpus, ready for training

[CORPUS] IGNORE

### long sentences are filtered out, since they slow down GIZA++ 
# and are a less reliable source of data. set here the maximum
# length of a sentence
#
max-sentence-length = 100


### command to run to get raw corpus files
#
# get-corpus-script = 

### raw corpus files (untokenized, but sentence aligned)
# 
#raw-stem = $nist-training/training/news-par.$pair-extension.utf8

### tokenized corpus files (may contain long sentences)
#
#tokenized-stem =

### if sentence filtering should be skipped,
# point to the clean training data
#
#clean-stem = 

### if corpus preparation should be skipped,
# point to the prepared training data
#
#lowercased-stem = 

#atb-trans.ar-en.utf8.en
#atb10.ar-en.en
#gale-p-all.ar-en.en
#gale-y1all.ar-en.en
#news-etirr.ar-en.en
#news-gale.ar-en.utf8.en
#news-par.ar-en.utf8.en
#news-trans.ar-en.utf8.en
#un.ar-en.utf8.ng.en

[CORPUS:amara] IGNORE
raw-stem = $amara-training/$pair-extension/amara.training

[CORPUS:iwslt-talk] 
raw-stem = $iwslt-training/training/train.notags.$pair-extension

#################################################################
# LANGUAGE MODEL TRAINING

[LM] 

### tool to be used for language model training
# for instance: ngram-count (SRILM), train-lm-on-disk.perl (Edinburgh) 
# 
lm-training = $srilm-dir/ngram-count
settings = "-interpolate -kndiscount -unk"
order = 5

### tool to be used for training randomized language model from scratch
# (more commonly, a SRILM is trained)
#
#rlm-training = "$moses-src-dir/randlm/bin/buildlm -falsepos 8 -values 8"

### script to use for binary table format for irstlm or kenlm
# (default: no binarization)

# irstlm
#lm-binarizer = $moses-src-dir/irstlm/bin/compile-lm

# kenlm, also set type to 8
#lm-binarizer = $moses-src-dir/bin/build_binary
type = 8

#
# if binarized, set type (default srilm; if binarized: irstlm)
#
# set to 8 when using kenlm
#type = 8

### script to create quantized language model format (irstlm)
# (default: no quantization)
# 
#lm-quantizer = $moses-src-dir/irstlm/bin/quantize-lm

### script to use for converting into randomized table format
# (default: no randomization)
#
#lm-randomizer = "$moses-src-dir/randlm/bin/buildlm -falsepos 8 -values 8"

### each language model to be used has its own section here

#[LM:europarl] IGNORE

### command to run to get raw corpus files
#
#get-corpus-script = ""

### raw corpus (untokenized)
#
#raw-corpus = $nist-training/training/europarl-v7.$output-extension

### tokenized corpus files (may contain long sentences)
#
#tokenized-corpus = 

### if corpus preparation should be skipped, 
# point to the prepared language model

#[LM:iwslt-talk]
#raw-corpus = $iwslt-training/training/train.notags.$pair-extension.$output-extension

[LM:lm-interpolated-target-side-tune-mt06-with-GigaWord]
lm = /home/guzmanhe/IWSLT2013/lm/english/ggw-un+news+ted+common+euro.tune.iwslt2010.lm.prob.kenlm

#################################################################
# INTERPOLATING LANGUAGE MODELS

[INTERPOLATED-LM] IGNORE

# if multiple language models are used, these may be combined
# by optimizing perplexity on a tuning set
# see, for instance [Koehn and Schwenk, IJCNLP 2008]

### script to interpolate language models
# if commented out, no interpolation is performed
#
#script = $moses-script-dir/ems/support/interpolate-lm.perl

### tuning set
# you may use the same set that is used for mert tuning (reference set)
#
#tuning-sgm = $nist-training/dev/newstest2010-ref.$output-extension.sgm
#raw-tuning =
#tokenized-tuning = 
#factored-tuning = 
#lowercased-tuning = 
#split-tuning = 

### script to use for binary table format for irstlm or kenlm
# (default: no binarization)

# irstlm
#lm-binarizer = $moses-src-dir/irstlm/bin/compile-lm

# kenlm, also set type to 8
#lm-binarizer = $moses-src-dir/bin/build_binary
#type = 8

### script to create quantized language model format (irstlm)
# (default: no quantization)
# 
#lm-quantizer = $moses-src-dir/irstlm/bin/quantize-lm

### script to use for converting into randomized table format
# (default: no randomization)
#
#lm-randomizer = "$moses-src-dir/randlm/bin/buildlm -falsepos 8 -values 8"

#################################################################
# MODIFIED MOORE LEWIS FILTERING

[MML] IGNORE

### specifications for language models to be trained
#
#lm-training = $srilm-dir/ngram-count
#lm-settings = "-interpolate -kndiscount -unk"
#lm-binarizer = $moses-src-dir/bin/build_binary
#lm-query = $moses-src-dir/bin/query
#order = 5

### in-/out-of-domain source/target corpora to train the 4 language model
# 
# in-domain: point either to a parallel corpus
#outdomain-stem = [CORPUS:toy:clean-split-stem]

# ... or to two separate monolingual corpora
#indomain-target = [LM:toy:lowercased-corpus]
#raw-indomain-source = $toy-data/nc-5k.$input-extension

# point to out-of-domain parallel corpus
#outdomain-stem = [CORPUS:giga:clean-split-stem]

# settings: number of lines sampled from the corpora to train each language model on
# (if used at all, should be small as a percentage of corpus)
#settings = "--line-count 100000"

#################################################################
# TRANSLATION MODEL TRAINING

[TRAINING]

### training script to be used: either a legacy script or 
# current moses training script (default) 
# 
script = $moses-script-dir/training/train-model.perl
corpus = $working-dir/baseline-hassan/corpus.56
### general options
#
training-options = "-mgiza -mgiza-cpus 10 -sort-buffer-size 20G"

### factored training: specify here which factors used
# if none specified, single factor training is assumed
# (one translation step, surface to surface)
#
#input-factors = word lemma pos morph
#output-factors = word lemma pos
#alignment-factors = "word -> word"
#translation-factors = "word -> word"
#reordering-factors = "word -> word"
#generation-factors = "word -> pos"
#decoding-steps = "t0, g0"


### pre-computation for giza++
# giza++ has a more efficient data structure that needs to be
# initialized with snt2cooc. if run in parallel, this may reduces
# memory requirements. set here the number of parts
#
run-giza-in-parts = 5

### symmetrization method to obtain word alignments from giza output
# (commonly used: grow-diag-final-and)
#
alignment-symmetrization-method = grow-diag-final-and


### use of Chris Dyer's fast align for word alignment
#
#fast-align-settings = "-d -o -v"


### use of berkeley aligner for word alignment
#
#use-berkeley = true
#alignment-symmetrization-method = berkeley
#berkeley-train = $moses-script-dir/ems/support/berkeley-train.sh
#berkeley-process =  $moses-script-dir/ems/support/berkeley-process.sh
#berkeley-jar = /your/path/to/berkeleyaligner-1.1/berkeleyaligner.jar
#berkeley-java-options = "-server -mx30000m -ea"
#berkeley-training-options = "-Main.iters 5 5 -EMWordAligner.numThreads 8"
#berkeley-process-options = "-EMWordAligner.numThreads 8"
#berkeley-posterior = 0.5

### if word alignment should be skipped,
# point to word alignment files
#
#word-alignment = $working-dir/model/aligned.1

### create a bilingual concordancer for the model
#
#biconcor = $moses-script-dir/ems/biconcor/biconcor

### lexicalized reordering: specify orientation type
# (default: only distance-based reordering model)
#
lexicalized-reordering = msd-bidirectional-fe

### hierarchical rule set
#
#hierarchical-rule-set = true

### settings for rule extraction
#
#extract-settings = ""

### unknown word labels (target syntax only)
# enables use of unknown word labels during decoding
# label file is generated during rule extraction
#
#use-unknown-word-labels = true

### if phrase extraction should be skipped,
# point to stem for extract files
#
# extracted-phrases = 

### settings for rule scoring
#
score-settings = "--KneserNey"

### include word alignment in phrase table
#
include-word-alignment-in-rules = yes

### if phrase table training should be skipped,
# point to phrase translation table
#
#phrase-translation-table = $working-dir/model/phrase-table.1.3

### if reordering table training should be skipped,
# point to reordering table
#
#reordering-table =  $working-dir/model/reordering-table.1.3.wbe-msd-bidirectional-fe.gz

### if training should be skipped, 
# point to a configuration file that contains
# pointers to all relevant model files
#
config = $working-dir/baseline-hassan/moses.ini.1.3.nodup
lexical-translation-table = $working-dir/baseline-hassan/lex.56


#####################################################
### TUNING: finding good weights for model components

[TUNING]

### instead of tuning with this setting, old weights may be recycled
# specify here an old configuration file with matching weights
#
weight-config = $working-dir/baseline-hassan//moses.tuned.ini.48

### tuning script to be used
#
#tuning-script = $moses-script-dir/training/mert-moses.pl
#tuning-settings = "-mertdir $moses-src-dir/bin --threads 24 --batch-mira --return-best-dev -maximum-iterations 25"

tuning-script = $moses-script-dir/training/mert-moses.pl
tuning-settings = "-mertdir $moses-src-dir/mert -threads=16 --pairwise-ranked --nbest=1000 --pairwise-ranked --proargs='--smooth-brevity-penalty'"

### specify the corpus used for tuning 
# it should contain 1000s of sentences
#
input-sgm = $iwslt-training/dev/IWSLT13.TED.dev2010.$pair-extension.$input-extension.xml
#tokenized-input = 
#factorized-input = 
#input =
# 
reference-sgm = $iwslt-training/dev/IWSLT13.TED.dev2010.$pair-extension.$output-extension.xml
#tokenized-reference = 
#factorized-reference = 
#reference = 
#tuning-input-from-sgm = no
#tuning-reference-from-sgm = no
### size of n-best list used (typically 100)
#
nbest = 100
#multiref = yes
### ranges for weights for random initialization
# if not specified, the tuning script will use generic ranges
# it is not clear, if this matters
#
# lambda = 

### additional flags for the filter script
#
filter-settings = ""

### additional flags for the decoder
#
decoder-settings = "-threads 16 -monotone-at-punctuation  -drop-unknown "

### if tuning should be skipped, specify this here
# and also point to a configuration file that contains
# pointers to all relevant model files
#
#config = $working-dir/tuning/moses.weight-reused.ini.1

#########################################################
## RECASER: restore case, this part only trains the model

[RECASING]

#decoder = $moses-src-dir/moses-cmd/src/moses.1521.srilm

### training data
# raw input needs to be still tokenized,
# also also tokenized input may be specified
#
#tokenized = [LM:europarl:tokenized-corpus]

# recase-config = 

#lm-training = $srilm-dir/ngram-count

#######################################################
## TRUECASER: train model to truecase corpora and input

[TRUECASER]

### script to train truecaser models
#
trainer = $moses-script-dir/recaser/train-truecaser.perl.2

### training data
# data on which truecaser is trained
# if no training data is specified, parallel corpus is used
#
# raw-stem = 
# tokenized-stem =

### trained model
#
truecase-model =  $working-dir/baseline-hassan/truecase-model.56
######################################################################
## EVALUATION: translating a test set using the tuned system and score it

[EVALUATION]

### number of jobs (if parallel execution on cluster)
#
#jobs = 10

### additional flags for the filter script
#
#filter-settings = ""

### additional decoder settings
# switches for the Moses decoder
#
decoder-settings = "-mbr -search-algorithm 1 -cube-pruning-pop-limit 5000 -s 5000 -threads 16 -monotone-at-punctuation  -drop-unknown "
### specify size of n-best list, if produced
#
#nbest = 100

### multiple reference translations
#
#multiref = yes

### prepare system output for scoring 
# this may include detokenization and wrapping output in sgm 
# (needed for nist-bleu, ter, meteor)
#

#recaser = $moses-script-dir/recaser/recase.perl
wrapping-script = "$moses-script-dir/ems/support/wrap-xml.perl $output-extension"
#output-sgm = 

### BLEU
#
nist-bleu = $moses-script-dir/generic/mteval-v13a.pl
nist-bleu-c = "$moses-script-dir/generic/mteval-v13a.pl -c"
#multi-bleu = $moses-script-dir/generic/multi-bleu.perl
#ibm-bleu =
multeval = /work/scoring/multeval-0.5.1/multeval_command.sh
multeval-tokenizer = "$moses-script-dir/tokenizer/tokenizer.perl -a -l $output-extension"

### TER: translation error rate (BBN metric) based on edit distance
# not yet integrated
#
# ter = 

### METEOR: gives credit to stem / worknet synonym matches
# not yet integrated
#
# meteor = 

### Analysis: carry out various forms of analysis on the output
#
##analysis = $moses-script-dir/ems/support/analysis.perl
#
# also report on input coverage
##analyze-coverage = yes
#
# also report on phrase mappings used
# no if using cdec or jane
##report-segmentation = yes
#
# report precision of translations for each input word, broken down by
# count of input word in corpus and model
#report-precision-by-coverage = yes
#
# further precision breakdown by factor
#precision-by-coverage-factor = pos

[EVALUATION:tst2010]

### input data
#
input-sgm = $iwslt-training/test/IWSLT13.TED.tst2010.$pair-extension.$input-extension.xml
### reference data
#
reference-sgm = $iwslt-training/test/IWSLT13.TED.tst2010.$pair-extension.$output-extension.xml

### analysis settings 
# may contain any of the general evaluation analysis settings
# specific setting: base coverage statistics on earlier run
#
#precision-by-coverage-base = $working-dir/evaluation/test.analysis.5

### wrapping frame
# for nist-bleu and other scoring scripts, the output needs to be wrapped 
# in sgm markup (typically like the input sgm)
#
#multiref = yes 

wrapping-frame = $input-sgm

[EVALUATION:iwslt13]
input-sgm = $iwslt-training/test2013/IWSLT13.TED.tst2013.$pair-extension.$input-extension.xml
reference-sgm = $iwslt-training/test2013/IWSLT13.TED.tst2013.$pair-extension.$output-extension.xml

wrapping-frame = $input-sgm

[EVALUATION:iwslt12]
input-sgm = $iwslt-training/test2012/IWSLT13.TED.tst2012.$pair-extension.$input-extension.xml
reference-sgm = $iwslt-training/test2012/IWSLT13.TED.tst2012.$pair-extension.$output-extension.xml

wrapping-frame = $input-sgm

##########################################
### REPORTING: summarize evaluation scores



[REPORTING]

### currently no parameters for reporting section

